{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing - Udacity Free Trial Screener\n",
    "## Dana Hagist\n",
    "\n",
    "## Experiment Overview\n",
    "\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This screenshot (https://drive.google.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view) shows what the experiment looks like.\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Choice\n",
    "\n",
    "### List of Metrics:\n",
    "\n",
    "- Number of cookies: That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "- Number of user-ids: That is, number of users who enroll in the free trial. (dmin=50)\n",
    "- Number of clicks: That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "- Click-through-probability: That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "- Gross conversion: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "- Retention: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "- Net conversion: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "\n",
    "#### Invariant Metrics\n",
    "Invariant metrics are those that should be evenly distributed between your treatment and control groups.  That is, they should be unaffected by the experiment.  The below three metrics all occur before the free trial screener is triggered, meaning they should be invariant:\n",
    "- Number of Cookies\n",
    "- Number of clicks\n",
    "- Click-through-probability \n",
    "\n",
    "#### Evaluation Metrics\n",
    "Evaluation metrics are those that you care to measure to understand the impact of your experiment.  In this case, the hope is that we are able to reduce the gross conversion (students enrolling in free trial) while keeping net conversion (students remaining enrolled past the 14-day boundary) consistent.  The intent here, as outlined in the Experiment Overview, is to free up coaching resources during the free trial period for those that are more likely to go on and complete the course, due to having ample time to dedicate.  Therefore, our Evaluation Metrics are as follows:\n",
    "- Gross Conversion: Should be reduced by 1%\n",
    "- Net Conversion: Should increase by 0.75%\n",
    "\n",
    "#### Other Metrics:\n",
    "The other two metrics we would not need to use for the scope of our analysis, which are Number of user ids and retention.  \n",
    "\n",
    "#### What we will be Looking for:\n",
    "The results we will be seeking are that the gross conversion rate has a statistically significant decrease by the miniumum magnitude of 1%, and the net conversion rate DOES NOT DECREASE significantly.  The minimum threshold for the net conversion rate having a meaningful change is 0.75%, but the intent of the change is primarily to reduce gross conversion and NOT REDUCE net conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Variability\n",
    "\n",
    "This spreadsheet (https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0) contains rough estimates of the baseline values for these metrics (again, these numbers have been changed from Udacity's true numbers).\n",
    "\n",
    "Because our evaluation metrics based on cookies, and the unit of diversion is cookie, we can use analytical estimates.  If they were different, we would want to consider using empirical estimates.\n",
    "\n",
    "Furthermore, we expect our metrics to follow binomial distributions, so in estimating the variance, we will use the formula for a binominal distribution, which is:\n",
    "p * (1-p) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.0\n"
     ]
    }
   ],
   "source": [
    "# Baseline values from spreadsheet - variables are in same order\n",
    "cookies_crs_ovrw_dly = 40000\n",
    "cookies_free_trl_dly = 3200\n",
    "enroll_per_day = 660\n",
    "click_thru_prob_free_trl = .08\n",
    "prob_enroll_click = .20625\n",
    "prob_pymt_enroll = .53\n",
    "prob_pymt_click = .1093125\n",
    "\n",
    "# Assuming overall visitors cookie sample of 5000,\n",
    "# We calculate the total number who are likely to click on free trial\n",
    "# This will be our experiment sample size\n",
    "N = 5000 * 3200/40000\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Gross Conversions:  32.0\n",
      "Gross Conversion Standard Deviation:  0.020230604137\n",
      "Expected Net Conversions:  43.725\n",
      "Net Conversion Standard Deviation:  0.0156015445825\n"
     ]
    }
   ],
   "source": [
    "# Importing Numpy for Square Root and other calculations\n",
    "import numpy as np\n",
    "\n",
    "# Expected Gross Conversions (based on 5000 sample)\n",
    "gross_conv_expect = N * click_thru_prob_free_trl\n",
    "# Standard deviation using variance calculation from above\n",
    "gross_sd = np.sqrt(prob_enroll_click * (1-prob_enroll_click) / N)\n",
    "# Printing expected gross conversions and standard deviation\n",
    "print(\"Expected Gross Conversions: \" , gross_conv_expect)\n",
    "print(\"Gross Conversion Standard Deviation: \" , gross_sd)\n",
    "\n",
    "# Expected Net Conversions (based on 5000 sample)\n",
    "net_conv_expect = N * prob_pymt_click\n",
    "net_sd = np.sqrt(prob_pymt_click * (1-prob_pymt_click) / N)\n",
    "# Printing expected net conversions and standard deviation\n",
    "print(\"Expected Net Conversions: \" , net_conv_expect)\n",
    "print(\"Net Conversion Standard Deviation: \" , net_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizing\n",
    "### Choosing Number of Samples given Power\n",
    "\n",
    "One thing that I need to check is how many total pageviews would be necessary in order to adequately power the experiment.  There is a good online calculator that can be used here (http://www.evanmiller.org/ab-testing/sample-size.html)\n",
    "\n",
    "For purposes of the analysis, we will use a significance level of 95% (alpha = .05) and beta = .2\n",
    "\n",
    "#### Gross Conversion\n",
    "- Baseline Conversion Rate: .20625\n",
    "- Minimum Detectable Effect: .01\n",
    "- Alpha = .05\n",
    "- Beta = .2\n",
    "- Required Sample Size (per group): 25,835\n",
    "- Total Sample Size = 25,835 * 2 = 51,670\n",
    "- Total Pageviews Required = 51,670 / .08 (click thru prob) =  645,875\n",
    "\n",
    "#### Net Conversion\n",
    "- Baseline Conversion Rate: .1093125\n",
    "- Minimum Detectable Effect: .0075\n",
    "- Alpha = .05\n",
    "- Beta = .2\n",
    "- Required Sample Size (per group): 27,411\n",
    "- Total Sample Size = 27,411 * 2 = 54,822\n",
    "- Total Pageviews Required = 54,877 / .08 = 685,963\n",
    "\n",
    "\n",
    "### Choosing Duration vs. Exposure\n",
    "\n",
    "Because this change is very low risk (not a reasonable expectation of harm to users or cost), I wouldn't see an issue running all traffic through the experiment.  \n",
    "\n",
    "Because we have approximately 40,000 visitors per day, and are seeking a total of 685,963 pageviews in order to have adequate power in our experiment, the duration of the experiment should be approximately 17.14 days (although we can do 18 to be safe).\n",
    "\n",
    "This seems to be an acceptable amount of time to run the experiment as with a two week timeframe, there are unlikely to be too many other factors that could impact our evaluation metrics.  However, it would likely be important to look over the course of a month to determine whether certain two week periods have higher gross or net conversion rates than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The data for you to analyze is here (https://www.google.com/url?q=https://docs.google.com/a/knowlabs.com/spreadsheets/d/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8/edit%23gid%3D0&sa=D&ust=1535379387782000). This data contains the raw information needed to compute the above metrics, broken down day by day. Note that there are two sheets within the spreadsheet - one for the experiment group, and one for the control group.\n",
    "\n",
    "However, for purposes of the analysis (and organization), I've created a single csv with a column denoting whether the observation was in the treatment group (treatment = 1) or control group (treatment = 0)\n",
    "\n",
    "The meaning of each column is:\n",
    "\n",
    "- Pageviews: Number of unique cookies to view the course overview page that day.\n",
    "- Clicks: Number of unique cookies to click the course overview page that day.\n",
    "- Enrollments: Number of user-ids to enroll in the free trial that day.\n",
    "- Payments: Number of user-ids who who enrolled on that day to remain enrolled for 14 days and thus make a payment. (Note that the date for this column is the start date, that is, the date of enrollment, rather than the date of the payment. The payment happened 14 days later. Because of this, the enrollments and payments are tracked for 14 fewer days than the other columns.)\n",
    "- Treatment: 1 for set of cookies in treatment group, and 0 for control group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Pageviews  Clicks  Enrollments  Payments  Treatment\n",
      "0  Sat, Oct 11       7723     687        134.0      70.0          0\n",
      "1  Sun, Oct 12       9102     779        147.0      70.0          0\n",
      "2  Mon, Oct 13      10511     909        167.0      95.0          0\n",
      "3  Tue, Oct 14       9871     836        156.0     105.0          0\n",
      "4  Wed, Oct 15      10014     837        163.0      64.0          0\n",
      "           Date  Pageviews  Clicks  Enrollments  Payments  Treatment\n",
      "37  Sat, Oct 11       7716     686        105.0      34.0          1\n",
      "38  Sun, Oct 12       9288     785        116.0      91.0          1\n",
      "39  Mon, Oct 13      10480     884        145.0      79.0          1\n",
      "40  Tue, Oct 14       9867     827        138.0      92.0          1\n",
      "41  Wed, Oct 15       9793     832        140.0      94.0          1\n"
     ]
    }
   ],
   "source": [
    "# Importing pandas to work with the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Reading in the data\n",
    "df = pd.read_csv('udacity_experiment.csv')\n",
    "\n",
    "# Creating control and experiment/treatment groups\n",
    "control = df[df['Treatment']==0]\n",
    "treatment = df[df['Treatment']==1]\n",
    "\n",
    "# Printing top rows for treatment and control groups\n",
    "print(control.head())\n",
    "print(treatment.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "For the invariant metrics, I need to ensure that they are evenly distributed between our treatment and control groups, within an acceptable significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cookie Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cookies in control group:  345543\n",
      "Number of cookies in treatment group:  344660\n",
      "Difference in cookies between treatment and control:  883\n",
      "Standard deviation:  0.000601840740294\n",
      "Margin of error: 0.00117960785098\n",
      "Lower bound of the 95% confidence interval:  0.498820392149\n",
      "Upper bound of the 95% confidence interval:  0.501179607851\n",
      "Oberved fraction:  0.500639666881\n"
     ]
    }
   ],
   "source": [
    "# Finding and printing number of cookies in control group\n",
    "cookies_page_control_cnt =sum(control.Pageviews)\n",
    "print(\"Number of cookies in control group: \", cookies_page_control_cnt)\n",
    "\n",
    "# Finding and printing number of cookies in treatment group\n",
    "cookies_page_treatment_cnt =sum(treatment.Pageviews)\n",
    "print(\"Number of cookies in treatment group: \", cookies_page_treatment_cnt)\n",
    "\n",
    "# Printing difference between the two groups\n",
    "print(\"Difference in cookies between treatment and control: \", cookies_page_control_cnt - cookies_page_treatment_cnt)\n",
    "\n",
    "# Finding standard deviation of cookie count\n",
    "cookie_cnt_sd = np.sqrt((0.5 * 0.5) / (cookies_page_control_cnt + cookies_page_treatment_cnt))\n",
    "print (\"Standard deviation: \",cookie_cnt_sd)\n",
    "\n",
    "# Finding margin of error for cookie count\n",
    "m= cookie_cnt_sd * 1.96\n",
    "print(\"Margin of error:\", m)\n",
    "\n",
    "# Printing lower bound as .5 - margin of error\n",
    "lower_bound = 0.5 - m\n",
    "print(\"Lower bound of the 95% confidence interval: \", lower_bound)\n",
    "\n",
    "# Printing upper bound as .5 + margin of error\n",
    "upper_bound = 0.5 + m\n",
    "print(\"Upper bound of the 95% confidence interval: \", upper_bound)\n",
    "\n",
    "# Printing the observed percentage of cookies count in control vs total \n",
    "p_observed = cookies_page_control_cnt / (cookies_page_control_cnt + cookies_page_treatment_cnt)\n",
    "print(\"Oberved fraction: \", p_observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above: \n",
    "Because each cookie should have a 50% chance of falling into the treatment or control group, we have a relatively small standard deviation and tight margin of error with a 95% confidence level. The allowable percentage of cookies in the control group would be up to .501 and our observation is .5006, so we are within the 95% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cookies to click 'start free trial' button in the control group: 28378\n",
      "Number of unique cookies to click 'start free trial' button in the treatment group: 28325\n",
      "Difference in cookies clicking free trial button between treatment and control:  53\n",
      "Standard deviation: 0.0020997470797\n",
      "Margin of error: 0.00411550427621\n",
      "Lower bound of the confidence interval: 0.495884495724\n",
      "Upper bound of the confidence interval: 0.504115504276\n",
      "Observed fraction: 0.500467347407\n"
     ]
    }
   ],
   "source": [
    "# Printing number of cookies to click 'start free trial' button in control group\n",
    "cookies_clicks_control_cnt = sum(control.Clicks)\n",
    "print(\"Number of unique cookies to click 'start free trial' button in the control group:\", cookies_clicks_control_cnt)\n",
    "\n",
    "# Printing number of cookies to click 'start free trial' button in treatment group\n",
    "cookies_clicks_treatment_cnt = sum(treatment.Clicks)\n",
    "print(\"Number of unique cookies to click 'start free trial' button in the treatment group:\", cookies_clicks_treatment_cnt)\n",
    "\n",
    "# Printing difference between treatment and control groups\n",
    "print(\"Difference in cookies clicking free trial button between treatment and control: \",\n",
    "      cookies_clicks_control_cnt - cookies_clicks_treatment_cnt)\n",
    "\n",
    "# Cookie click standard deviation\n",
    "cookie_click_sd = np.sqrt( (0.5 * 0.5) / (cookies_clicks_control_cnt + cookies_clicks_treatment_cnt))\n",
    "print(\"Standard deviation:\", cookie_click_sd)\n",
    "\n",
    "# Margin of Error\n",
    "m = cookie_click_sd * 1.96\n",
    "print(\"Margin of error:\", m)\n",
    "\n",
    "# Printing lower bound as .5 - margin of error\n",
    "lower_bound = 0.5 - m\n",
    "print(\"Lower bound of the confidence interval:\", lower_bound)\n",
    "\n",
    "\n",
    "# Printing upper bound as .5 + margin of error\n",
    "upper_bound = 0.5 + m\n",
    "print(\"Upper bound of the confidence interval:\", upper_bound)\n",
    "\n",
    "# Printing the observed percentage of cookies cliking on free trial in control vs total \n",
    "p_observed = cookies_clicks_control_cnt / (cookies_clicks_control_cnt + cookies_clicks_treatment_cnt)\n",
    "print(\"Observed fraction:\", p_observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above: \n",
    "Because each cookie clicking on the 'start free trial' button should have a 50% chance of falling into the treatment or control group, we have a relatively small standard deviation and tight margin of error with a 95% confidence level. The allowable percentage of cookies in the control group would be up to .504 and our observation is .5004, so we are within the 95% confidence interval.  No issues noted with this invariant metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click-Through Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click-through probability in control group:  0.0821258135746\n",
      "Standard Deviation:  0.000467068276555\n",
      "Lower bound:  0.0812103597525\n",
      "Upper bound:  0.0830412673966\n",
      "Click-through probability in treatment group:  0.0821824406662\n"
     ]
    }
   ],
   "source": [
    "# Calculating the click-through probability in control group\n",
    "# Remember this is total cookies that clicked 'start free trial' / cookies that saw course overview\n",
    "ctp = cookies_clicks_control_cnt / cookies_page_control_cnt\n",
    "print(\"Click-through probability in control group: \", ctp)\n",
    "\n",
    "# Calculating and printing standard deviation of click-through probability \n",
    "ctp_sd = np.sqrt((ctp * (1-ctp)) / (cookies_page_control_cnt))\n",
    "print(\"Standard Deviation: \", ctp_sd)\n",
    "\n",
    "# Calculating margin of error\n",
    "m = ctp_sd * 1.96\n",
    "\n",
    "# Lower bound for 95% confidence interval\n",
    "lower_bound= ctp - m\n",
    "print(\"Lower bound: \", lower_bound)\n",
    "\n",
    "# Upper bound for 95% confidence interval\n",
    "upper_bound= ctp + m\n",
    "print(\"Upper bound: \", upper_bound)\n",
    "\n",
    "# Calculating click-through probability of treatment group, which should be in 95% confidence interval of control group\n",
    "p_treatment = cookies_clicks_treatment_cnt / cookies_page_treatment_cnt\n",
    "print(\"Click-through probability in treatment group: \", p_treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above:\n",
    "\n",
    "In the above, we are ensuring that the probability of a cookie clicking the 'start free trial' in the treatment is within the 95% confidence interval of the same probability in control group.  We can see that 0.0822 falls within the acceptable threshold of .0812 and .0830, so no issues here.\n",
    "\n",
    "We have no tested all of our invariant metrics, which fall within the acceptable thresholds to support that the control and experiment groups are indeed comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Practical and Statistical Significance\n",
    "\n",
    "Next, for my evaluation metrics, I will calculate a confidence interval for the difference between the experiment and control groups, and check whether each metric is statistically and/or practically significant. \n",
    "\n",
    "A metric is statistically significant if the confidence interval does not include 0 (that is, we can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, we can be confident there is a change that matters to the business.)\n",
    "\n",
    "Because I have chosen two evaluation metrics and we are looking for both to be statistically significant, we don't need to use a Bonferroni correction.  A Bonferroni correction can be useful in cases where we are looking for ANY statistically significant result from a number of metrics.  The reason for this is that the more metrics you are using for evaluation, the more likely you are to get at least one statistically significant result by chance.  An example is that when using a 95% confidence interval, out of 20 metrics, one is likely to show up as statistically significant by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Nulls in Control--\n",
      "Date           0\n",
      "Pageviews      0\n",
      "Clicks         0\n",
      "Enrollments    0\n",
      "Payments       0\n",
      "Treatment      0\n",
      "dtype: int64\n",
      "--Nulls in Treatment--\n",
      "Date           0\n",
      "Pageviews      0\n",
      "Clicks         0\n",
      "Enrollments    0\n",
      "Payments       0\n",
      "Treatment      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Need to drop the records where we don't have number of enrollments and payments\n",
    "control2=control[control.Enrollments.isnull() == False]\n",
    "treatment2=treatment[treatment.Enrollments.isnull() == False]\n",
    "\n",
    "# Checking for any more remaining null values\n",
    "print(\"--Nulls in Control--\")\n",
    "print(control2.isnull().sum())\n",
    "print(\"--Nulls in Treatment--\")\n",
    "print(treatment2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Gross Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross conversion for control group:  0.218874689181\n",
      "Gross conversion for experiment group:  0.1983198146\n",
      "Pooled probability:  0.208607067404\n",
      "Pool standard error:  0.00437167538523\n",
      "Margin of error:  0.00856848375504\n",
      "Difference observed between control and experiment group  -0.0205548745804\n",
      "Lower bound of the 95% of the confidence intervall:  -0.0291233583354\n",
      "Upper bound of the 95% of the confidence intervall:  -0.0119863908253\n"
     ]
    }
   ],
   "source": [
    "# Calculating Control Sample and Enrollment Count\n",
    "N_cont = sum(control2.Clicks)\n",
    "X_cont = sum(control2.Enrollments)\n",
    "\n",
    "# Calculating Experiment Sample and Enrollment Count\n",
    "N_exp=sum(treatment2.Clicks)\n",
    "X_exp=sum(treatment2.Enrollments)\n",
    "\n",
    "# Printing gross conversion for both control and experiment group\n",
    "print(\"Gross conversion for control group: \", X_cont / N_cont)\n",
    "print(\"Gross conversion for experiment group: \", X_exp / N_exp)\n",
    "\n",
    "# Calculating the pooled probability (all enrollments divided by clicks on 'start free trial')\n",
    "pooled_prob = (X_cont + X_exp) / (N_cont + N_exp)\n",
    "print(\"Pooled probability: \", pooled_prob)\n",
    "\n",
    "# Calculating the pooled standard error\n",
    "pooled_SE = np.sqrt(pooled_prob * (1 - pooled_prob) * (1/N_cont + 1/N_exp))\n",
    "print(\"Pool standard error: \", pooled_SE)\n",
    "\n",
    "# Calculating the margine of error\n",
    "m = 1.96 * pooled_SE\n",
    "print(\"Margin of error: \", m)\n",
    "\n",
    "# Calculating the observed difference bewteen treatment and control\n",
    "diff_observed= X_exp/N_exp - X_cont/N_cont\n",
    "print(\"Difference observed between control and experiment group \", diff_observed)\n",
    "\n",
    "# Calculating lower and upper bounds of the confidence interval\n",
    "lower_bound = diff_observed - m\n",
    "print(\"Lower bound of the 95% of the confidence intervall: \", lower_bound)\n",
    "upper_bound = diff_observed + m\n",
    "print(\"Upper bound of the 95% of the confidence intervall: \", upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above\n",
    "In the above, we are calculating the observed difference between our treatment and control groups, and then determining the 95% confidence interval around that difference.  In this case, we can see that the interval does not include 0, meaning that our result is statistically significant.  Furthermore, as our observed difference, as well as the lower and the upper bound are lower than the minimum threshold (which is meant to be negative), the results are also practically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Net Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net conversion for control group:  0.117562019314\n",
      "Net conversion for experiment group:  0.11268829664\n",
      "Pooled probability:  0.115127485312\n",
      "Pool standard error:  0.00343413351293\n",
      "Margin of error:  0.00673090168535\n",
      "Difference observed between control and experiment group  -0.00487372267454\n",
      "Lower bound of the 95% of the confidence intervall:  -0.0116046243599\n",
      "Upper bound of the 95% of the confidence intervall:  0.0018571790108\n"
     ]
    }
   ],
   "source": [
    "# Calculating Control Sample and Payment Count\n",
    "N_cont = sum(control2.Clicks)\n",
    "X_cont = sum(control2.Payments)\n",
    "\n",
    "# Calculating Experiment Sample and Payment Count\n",
    "N_exp=sum(treatment2.Clicks)\n",
    "X_exp=sum(treatment2.Payments)\n",
    "\n",
    "# Printing gross conversion for both control and experiment group\n",
    "print(\"Net conversion for control group: \", X_cont / N_cont)\n",
    "print(\"Net conversion for experiment group: \", X_exp / N_exp)\n",
    "\n",
    "# Calculating the pooled probability (all students who go on to pay divided by clicks on 'start free trial')\n",
    "pooled_prob = (X_cont + X_exp) / (N_cont + N_exp)\n",
    "print(\"Pooled probability: \", pooled_prob)\n",
    "\n",
    "# Calculating the pooled standard error\n",
    "pooled_SE = np.sqrt(pooled_prob * (1 - pooled_prob) * (1/N_cont + 1/N_exp))\n",
    "print(\"Pool standard error: \", pooled_SE)\n",
    "\n",
    "# Calculating the margine of error\n",
    "m = 1.96 * pooled_SE\n",
    "print(\"Margin of error: \", m)\n",
    "\n",
    "# Calculating the observed difference bewteen treatment and control\n",
    "diff_observed= X_exp/N_exp - X_cont/N_cont\n",
    "print(\"Difference observed between control and experiment group \", diff_observed)\n",
    "\n",
    "# Calculating lower and upper bounds of the confidence interval\n",
    "lower_bound = diff_observed - m\n",
    "print(\"Lower bound of the 95% of the confidence intervall: \", lower_bound)\n",
    "upper_bound = diff_observed + m\n",
    "print(\"Upper bound of the 95% of the confidence intervall: \", upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above\n",
    "In the above, once again we are calculating the observed difference between our treatment and control groups, and then determining the 95% confidence interval around that difference.  In this case, we can see that the interval DOES include 0, meaning that our result is NOT statistically significant.  It was also discussed early on that a magnitude (practical significance level) we would care about in this case is 0.75% or .0075, and although the lower bound of our confidence interval does contain that value, our observed difference is not that large of a magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Sign Tests\n",
    "\n",
    "Next, for each evaluation metric, I will do a sign test using the day-by-day breakdown. If the sign test does not agree with the confidence interval for the difference, I will see if I can figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the gross conversion for treatment and control groups\n",
    "gross_conversion_control = control2.Enrollments/control2.Clicks\n",
    "gross_conversion_treatment = treatment2.Enrollments/treatment2.Clicks\n",
    "# Some manipulation to reset the indexes of the above two series\n",
    "gross_conversion_control.reset_index(drop=True, inplace=True)\n",
    "gross_conversion_treatment.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Creating a dataframe for the sign test results\n",
    "sign_test_df = pd.concat([gross_conversion_control, gross_conversion_treatment], axis = 1)\n",
    "# Creating the two columns for control and treatment\n",
    "sign_test_df.columns = ['gross_conversion_control','gross_conversion_treatment']\n",
    "# Adding column to flag days where \n",
    "sign_test_df['sign_test']= sign_test_df.gross_conversion_control < sign_test_df.gross_conversion_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gross_conversion_control  gross_conversion_treatment  sign_test\n",
      "0                   0.195051                    0.153061      False\n",
      "1                   0.188703                    0.147771      False\n",
      "2                   0.183718                    0.164027      False\n",
      "3                   0.186603                    0.166868      False\n",
      "4                   0.194743                    0.168269      False\n",
      "5                   0.167679                    0.163706      False\n",
      "6                   0.195187                    0.162821      False\n",
      "7                   0.174051                    0.144172      False\n",
      "8                   0.189580                    0.172166      False\n",
      "9                   0.191638                    0.177907      False\n",
      "10                  0.226067                    0.165509      False\n",
      "11                  0.193317                    0.159800      False\n",
      "12                  0.190977                    0.190031      False\n",
      "13                  0.326895                    0.278336      False\n",
      "14                  0.254703                    0.189836      False\n",
      "15                  0.227401                    0.220779      False\n",
      "16                  0.306983                    0.276265      False\n",
      "17                  0.209239                    0.220109       True\n",
      "18                  0.265223                    0.276479       True\n",
      "19                  0.227520                    0.284341       True\n",
      "20                  0.246459                    0.252078       True\n",
      "21                  0.229075                    0.204317      False\n",
      "22                  0.297258                    0.251381      False\n",
      "Total Days:  23\n",
      "Number of days the gross conversion rate is higher for the experiment group than the control group: 4\n"
     ]
    }
   ],
   "source": [
    "# Printing subset of results\n",
    "print(sign_test_df)\n",
    "\n",
    "# Counting and printing number of days of experiment\n",
    "total_days = len(sign_test_df)\n",
    "print(\"Total Days: \", total_days)\n",
    "\n",
    "# Calculating number of days where experiment > control\n",
    "true_days = sum(sign_test_df.sign_test == True)\n",
    "\n",
    "# Printing number of days the gross conversion rate is higher for experiment than control\n",
    "print(\"Number of days the gross conversion rate is higher for the experiment \\\n",
    "group than the control group:\", true_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Sign Test Statistical Significance\n",
    "Using following online calculator to calculate statistical significance (http://graphpad.com/quickcalcs/binomial1.cfm)\n",
    "\n",
    "Successes = 4\n",
    "\n",
    "Experiments = 23\n",
    "\n",
    "Probability = 0.5\n",
    "\n",
    "The two-tail P-value returned by the calculator is equal to 0.0026, which is statistically significant.\n",
    "\n",
    "The sign test agrees with the effect size test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the gross conversion for treatment and control groups\n",
    "net_conversion_control = control2.Payments/control2.Clicks\n",
    "net_conversion_treatment = treatment2.Payments/treatment2.Clicks\n",
    "# Some manipulation to reset the indexes of the above two series\n",
    "net_conversion_control.reset_index(drop=True, inplace=True)\n",
    "net_conversion_treatment.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Creating a dataframe for the sign test results\n",
    "sign_test_df = pd.concat([net_conversion_control, net_conversion_treatment], axis = 1)\n",
    "# Creating the two columns for control and treatment\n",
    "sign_test_df.columns = ['net_conversion_control','net_conversion_treatment']\n",
    "# Adding column to flag days where \n",
    "sign_test_df['sign_test']= sign_test_df.net_conversion_control < sign_test_df.net_conversion_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    net_conversion_control  net_conversion_treatment  sign_test\n",
      "0                 0.101892                  0.049563      False\n",
      "1                 0.089859                  0.115924       True\n",
      "2                 0.104510                  0.089367      False\n",
      "3                 0.125598                  0.111245      False\n",
      "4                 0.076464                  0.112981       True\n",
      "5                 0.099635                  0.077411      False\n",
      "6                 0.101604                  0.056410      False\n",
      "7                 0.110759                  0.095092      False\n",
      "8                 0.086831                  0.110473       True\n",
      "9                 0.112660                  0.113953       True\n",
      "10                0.121107                  0.082176      False\n",
      "11                0.109785                  0.087391      False\n",
      "12                0.084211                  0.105919       True\n",
      "13                0.181278                  0.134864      False\n",
      "14                0.185239                  0.121076      False\n",
      "15                0.146893                  0.145743      False\n",
      "16                0.163373                  0.154345      False\n",
      "17                0.123641                  0.163043       True\n",
      "18                0.116373                  0.132050       True\n",
      "19                0.102180                  0.092033      False\n",
      "20                0.143059                  0.170360       True\n",
      "21                0.136564                  0.143885       True\n",
      "22                0.096681                  0.142265       True\n",
      "Total Days:  23\n",
      "Number of days the net conversion rate is higher for the experiment group than the control group: 10\n"
     ]
    }
   ],
   "source": [
    "# Printing subset of results\n",
    "print(sign_test_df)\n",
    "\n",
    "# Counting and printing number of days of experiment\n",
    "total_days = len(sign_test_df)\n",
    "print(\"Total Days: \", total_days)\n",
    "\n",
    "# Calculating number of days where experiment > control\n",
    "true_days = sum(sign_test_df.sign_test == True)\n",
    "\n",
    "# Printing number of days the net conversion rate is higher for experiment than control\n",
    "print(\"Number of days the net conversion rate is higher for the experiment \\\n",
    "group than the control group:\", true_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Sign Test Statistical Significance\n",
    "\n",
    "Using following online calculator to calculate statistical significance (http://graphpad.com/quickcalcs/binomial1.cfm)\n",
    "\n",
    "Successes = 10\n",
    "\n",
    "Experiments = 23\n",
    "\n",
    "Probability = 0.5\n",
    "\n",
    "The two-tail P-value returned by the calculator is equal to 0.6776, which is NOT statistically significant.\n",
    "\n",
    "The sign test agrees with the effect size test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recommendation\n",
    "\n",
    "Finally, it is time to make a recommendation.\n",
    "\n",
    "What we found during this experiment is that including the question about time available for study after clicking the 'start free trial' button did reduce the free-trial enrollments in a meaningful way (estimated at around 2%).  This result was statistically significant.\n",
    "\n",
    "However, the net conversion rate (those who go on to pay) also reduced. Although the reduction was not statistically significant, the 95% confidence interval did include the magnitude which we would consider meaningful on the negative side.\n",
    "\n",
    "In conclusion, it would probably make sense to run a follow-up experiment, perhaps including messaging about the benefits of paying and completing the course (to help with the net conversion rate issue).  Or, we could revisit the threshold for net conversions.  Either way, it's probably not good to launch the change at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibility for Follow-Up Experiment: How to Reduce Early Cancellations\n",
    "\n",
    "If you wanted to reduce the number of frustrated students who cancel early in the course, what experiment would you try? Give a brief description of the change you would make, what your hypothesis would be about the effect of the change, what metrics you would want to measure, and what unit of diversion you would use. Include an explanation of each of your choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
